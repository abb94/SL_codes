{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b85f03-a6f1-4e89-82f7-9ae9415191b8",
   "metadata": {},
   "source": [
    "# Deployment without Landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9135dc64-d330-4798-b57d-17458bda30e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load the saved model and move it to the GPU\n",
    "model = torch.load('C:/Users/Bruss/Desktop/Speciale/models/effecientnet_models/current_model_effecientnet_without_landmarks_18.pt')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the preprocessing transforms\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4429, 0.3043, 0.2806], \n",
    "                          std=[0.1187, 0.0874, 0.0728]),])\n",
    "\n",
    "# Capture images from the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        # Convert the image to RGB format\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Preprocess the image\n",
    "        pil_image = Image.fromarray(frame)\n",
    "        image_tensor = preprocess(pil_image)\n",
    "        image_tensor = image_tensor.unsqueeze(0)\n",
    "\n",
    "        # Move the data to the GPU\n",
    "        image_tensor = image_tensor.to(device)\n",
    "\n",
    "        # Pass the image through the model\n",
    "        with torch.no_grad():\n",
    "            output = model(image_tensor)\n",
    "\n",
    "        # Move the output to the CPU\n",
    "        output = output.cpu()\n",
    "\n",
    "        # Get the predicted class\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        predicted_class = predicted.item()\n",
    "\n",
    "        # Define the class names\n",
    "        class_names = ['afraid',  'alone', 'boss', 'hello', 'tough']\n",
    "\n",
    "        # Display the predicted class on the frame\n",
    "        predicted_class_name = class_names[predicted_class]\n",
    "        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "\n",
    "        # Show the frame\n",
    "        cv2.imshow('frame', frame)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac07399-69ce-459f-9688-884a0d921d22",
   "metadata": {},
   "source": [
    "# Deployment with Landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3580d801-d0bb-4701-adbd-e2adc1301e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Load the saved model and move it to the GPU\n",
    "model = torch.load('C:/Users/Bruss/Desktop/Speciale/models/effecientnet_models/current_model_effecientnet_with_landmarks_14.pt')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Define the preprocessing transforms\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4429, 0.3043, 0.2806], \n",
    "                          std=[0.1187, 0.0874, 0.0728]),])\n",
    "\n",
    "# Create a VideoCapture object to capture the video from the default camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize the frame counter and the start time\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a Mediapipe Hands and Face Detection objects\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "\n",
    "# Create a Mediapipe Hands and Face Detection objects\n",
    "\n",
    "hands = mp_hands.Hands()\n",
    "face_detection = mp_face_detection.FaceDetection()\n",
    "\n",
    "\n",
    "# Loop through each frame in the video stream\n",
    "while True:\n",
    "    # Read a frame from the video stream\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Increment the frame counter\n",
    "    frame_count += 1\n",
    "    \n",
    "    \n",
    "\n",
    "    # Flip the frame horizontally for a mirror-like effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Convert the image to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect hands\n",
    "    hands_results = hands.process(frame_rgb)\n",
    "\n",
    "    # Detect faces\n",
    "    face_detection_results = face_detection.process(frame_rgb)\n",
    "\n",
    "    # Draw landmarks for hands\n",
    "    if hands_results.multi_hand_landmarks:\n",
    "        for hand_landmarks in hands_results.multi_hand_landmarks:\n",
    "            mp.solutions.drawing_utils.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Draw landmarks for faces\n",
    "    if face_detection_results.detections:\n",
    "        for detection in face_detection_results.detections:\n",
    "            mp.solutions.drawing_utils.draw_detection(\n",
    "                frame,\n",
    "                detection)\n",
    "            \n",
    "    elapsed_time = time.time() - start_time\n",
    "    fps = frame_count / elapsed_time\n",
    "    cv2.putText(frame, \"FPS: {:.2f}\".format(fps), (10, 30),\n",
    "    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    \n",
    "    # Preprocess the image\n",
    "    pil_image = Image.fromarray(frame)\n",
    "    image_tensor = preprocess(pil_image)\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "\n",
    "    # Move the data to the GPU\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    # Pass the image through the model\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "\n",
    "    # Move the output to the CPU\n",
    "    output = output.cpu()\n",
    "\n",
    "    # Get the predicted class\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    predicted_class = predicted.item()\n",
    "\n",
    "    # Define the class names\n",
    "    class_names = ['afraid',  'alone', 'boss', 'hello', 'tough']\n",
    "\n",
    "    # Display the predicted class on the frame\n",
    "    predicted_class_name = class_names[predicted_class]\n",
    "    cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('frame', frame)\n",
    "    \n",
    "    # Exit if the user presses the 'q' key\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "\n",
    "# Release the VideoCapture object and close the OpenCV window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
